{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAS20fuhEBqW"
      },
      "source": [
        "# 3. Document Classification Based on BOW\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M159n1s3EBqa"
      },
      "source": [
        "## 1. Preparing the 20 Newsgroups Data and Feature Extraction\n",
        "\n",
        " http://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP8xqhbdEBqc",
        "outputId": "e538f233-b382-4368-b475-002101e2bbd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#Train set size: 2034\n",
            "#Test set size: 1353\n",
            "#Selected categories: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
            "#Train labels: {0, 1, 2, 3}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Create a list of topics to select from the 20 categories\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "\n",
        "# Fetch the training dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',\n",
        "# Remove hinting parts from the email content - classify purely based on content\n",
        "                                      remove=('headers', 'footers', 'quotes'),\n",
        "                                      categories=categories)\n",
        "\n",
        "# Fetch the test dataset\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',\n",
        "                                     remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=categories)\n",
        "\n",
        "print('#Train set size:', len(newsgroups_train.data))\n",
        "print('#Test set size:', len(newsgroups_test.data))\n",
        "print('#Selected categories:', newsgroups_train.target_names)\n",
        "print('#Train labels:', set(newsgroups_train.target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyOgG7L3EBqh",
        "outputId": "563bb290-8e39-4d01-ca88-e71646c3f158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#Train set text samples: Hi,\n",
            "\n",
            "I've noticed that if you only save a model (with all your mapping planes\n",
            "positioned carefully) to a .3DS file that when you reload it after restarting\n",
            "3DS, they are given a default position and orientation.  But if you save\n",
            "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
            "know why this information is not stored in the .3DS file?  Nothing is\n",
            "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
            "I'd like to be able to read the texture rule information, does anyone have \n",
            "the format for the .PRJ file?\n",
            "\n",
            "Is the .CEL file format available from somewhere?\n",
            "\n",
            "Rych\n",
            "#Train set label smaples: 1\n",
            "#Test set text samples: TRry the SKywatch project in  Arizona.\n",
            "#Test set label smaples: 2\n"
          ]
        }
      ],
      "source": [
        "print('#Train set text samples:', newsgroups_train.data[0])\n",
        "print('#Train set label smaples:', newsgroups_train.target[0])\n",
        "print('#Test set text samples:', newsgroups_test.data[0])\n",
        "print('#Test set label smaples:', newsgroups_test.target[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kJl9NpjJEBqj"
      },
      "outputs": [],
      "source": [
        "X_train = newsgroups_train.data   # Training dataset documents\n",
        "y_train = newsgroups_train.target # Training dataset labels\n",
        "\n",
        "X_test = newsgroups_test.data     # Test dataset documents\n",
        "y_test = newsgroups_test.target   # Test dataset labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icIDPfDrDAL1"
      },
      "source": [
        "## 2. Document Representation Based on Distributed Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QEQdqWrV166"
      },
      "source": [
        "### 1) Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YuEgpDpD9d2",
        "outputId": "5876d575-3456-4bdf-be3f-038225f35fb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/minjoo/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/minjoo/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for Word2Vec and machine learning models\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK resources for tokenization and stopwords\n",
        "nltk.download('punkt')  # For word tokenization\n",
        "nltk.download('stopwords')  # For filtering out common stopwords\n",
        "\n",
        "# Initialize a set of stopwords for English and a stemmer to reduce words to their root form\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()  # PorterStemmer is commonly used to reduce words to their stem form\n",
        "\n",
        "# Data preprocessing function\n",
        "def preprocess_data(data):\n",
        "    processed_data = []\n",
        "    for sentence in data:\n",
        "        # Tokenize the sentence into words\n",
        "        tokens = word_tokenize(sentence)\n",
        "        # Convert to lowercase, remove stopwords and special characters, and apply stemming\n",
        "        tokens = [stemmer.stem(re.sub(r'\\W+', '', word.lower()))\n",
        "                  for word in tokens\n",
        "                  if word.lower() not in stop_words and re.sub(r'\\W+', '', word)]\n",
        "        processed_data.append(tokens)  # Add the cleaned tokens to the processed data\n",
        "    return processed_data\n",
        "\n",
        "# Step 1: Train the Word2Vec model\n",
        "# Preprocess the training and testing data using the preprocess_data function\n",
        "X_train_tokenized = preprocess_data(X_train)\n",
        "X_test_tokenized = preprocess_data(X_test)\n",
        "\n",
        "# Flatten the tokenized training data into a single list and calculate word frequency\n",
        "all_words = [word for sentence in X_train_tokenized for word in sentence]\n",
        "word_counts = Counter(all_words)  # Count frequency of each word in the training data\n",
        "\n",
        "# Define a threshold to remove low-frequency words\n",
        "min_count_threshold = 2  # Words with a frequency of 2 or lower will be removed\n",
        "# Filter the tokenized training and testing data to keep only frequent words\n",
        "X_train_tokenized = [[word for word in sentence if word_counts[word] > min_count_threshold] for sentence in X_train_tokenized]\n",
        "X_test_tokenized = [[word for word in sentence if word_counts[word] > min_count_threshold] for sentence in X_test_tokenized]\n",
        "\n",
        "# Train the Word2Vec model with the tokenized and filtered training data\n",
        "# vector_size: Dimensionality of the word vectors\n",
        "# window: Maximum distance between the current and predicted word\n",
        "# sg: Use skip-gram (1) instead of CBOW (0)\n",
        "w2v_model = Word2Vec(sentences=X_train_tokenized, vector_size=100, window=2, min_count=2, sg=1)\n",
        "\n",
        "# Step 2: Function to generate Word2Vec vectors for each sentence\n",
        "# Each sentence vector is computed as the average of its word vectors\n",
        "def get_w2v_vectors(data, model, vector_size=100):\n",
        "    vectors = []\n",
        "    for sentence in data:\n",
        "        # Initialize a zero vector for the sentence\n",
        "        sentence_vec = np.zeros(vector_size)\n",
        "        count = 0  # Track the number of words found in the Word2Vec model\n",
        "        for word in sentence:\n",
        "            # Check if the word exists in the Word2Vec model\n",
        "            if word in model.wv.key_to_index:\n",
        "                # Add the word vector to the sentence vector\n",
        "                sentence_vec += model.wv[word]\n",
        "                count += 1\n",
        "        # If the sentence contains valid words, compute the average of the word vectors\n",
        "        if count != 0:\n",
        "            # The sentence vector is the average of the word vectors in the sentence\n",
        "            sentence_vec /= count\n",
        "        # Append the resulting sentence vector (average of word vectors) to the list\n",
        "        vectors.append(sentence_vec)\n",
        "    return np.array(vectors)\n",
        "\n",
        "# Generate Word2Vec vectors for both the training and testing data\n",
        "X_train_w2v = get_w2v_vectors(X_train_tokenized, w2v_model)\n",
        "X_test_w2v = get_w2v_vectors(X_test_tokenized, w2v_model)\n",
        "\n",
        "# Step 3: Train machine learning models (Logistic Regression and Random Forest)\n",
        "# Initialize two different classifiers for comparison\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),  # Logistic Regression with maximum iterations set to 1000\n",
        "    'Random Forest': RandomForestClassifier()  # Random Forest Classifier\n",
        "}\n",
        "\n",
        "# Dictionary to store model names and their performance metrics\n",
        "results = {'Model': [], 'Train Accuracy': [], 'Test Accuracy': []}\n",
        "\n",
        "# Train each model and evaluate accuracy on the training and testing data\n",
        "for model_name, model in models.items():\n",
        "    # Fit the model using the Word2Vec vectorized training data and corresponding labels\n",
        "    model.fit(X_train_w2v, y_train)\n",
        "    # Calculate accuracy on both the training and testing datasets\n",
        "    train_acc = model.score(X_train_w2v, y_train)\n",
        "    test_acc = model.score(X_test_w2v, y_test)\n",
        "\n",
        "    # Store the results for each model\n",
        "    results['Model'].append(model_name + \" (Word2Vec)\")  # Add the model name and the method used (Word2Vec)\n",
        "    results['Train Accuracy'].append(train_acc)  # Training accuracy\n",
        "    results['Test Accuracy'].append(test_acc)  # Testing accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yniy_RswYVfi",
        "outputId": "21eb9a21-10a9-4f88-ca83-fb5933937bf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Model': ['Logistic Regression (Word2Vec)', 'Random Forest (Word2Vec)'], 'Train Accuracy': [0.683874139626352, 0.9783677482792527], 'Test Accuracy': [0.6400591278640059, 0.6577974870657798]}\n"
          ]
        }
      ],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v64oR3mAWkQ5"
      },
      "source": [
        "### 2) FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1M5QWueHVzRr"
      },
      "outputs": [],
      "source": [
        "# Import FastText from Gensim library\n",
        "from gensim.models import FastText\n",
        "\n",
        "# Step 1: Train FastText model\n",
        "# FastText model is trained on tokenized training sentences\n",
        "# vector_size: Dimensionality of the word vectors\n",
        "# window: The maximum distance between the current and predicted word within a sentence\n",
        "# min_count: Ignores all words with total frequency lower than this value\n",
        "# sg: Training algorithm. 1 means skip-gram, 0 means CBOW\n",
        "fasttext_model = FastText(sentences=X_train_tokenized, vector_size=100, window=5, min_count=2, sg=1)\n",
        "\n",
        "# Step 2: Function to generate sentence vectors from Word2Vec or FastText models\n",
        "# This function takes tokenized sentences and converts them into sentence vectors\n",
        "# by averaging the word vectors for words that exist in the model's vocabulary.\n",
        "def get_w2v_vectors(data, model, vector_size=100):\n",
        "    vectors = []\n",
        "    for sentence in data:\n",
        "        # Initialize a zero vector for each sentence\n",
        "        sentence_vec = np.zeros(vector_size)\n",
        "        count = 0  # To track how many words in the sentence exist in the model\n",
        "        for word in sentence:\n",
        "            # Check if the word exists in the model's vocabulary\n",
        "            if word in model.wv.key_to_index:\n",
        "                # Add the word vector to the sentence vector\n",
        "                sentence_vec += model.wv[word]\n",
        "                count += 1\n",
        "        # If there are valid words in the sentence, compute the average word vector\n",
        "        if count != 0:\n",
        "            # The sentence vector is the average of the word vectors\n",
        "            sentence_vec /= count\n",
        "        # Append the sentence vector to the list\n",
        "        vectors.append(sentence_vec)\n",
        "    # Return the list of sentence vectors as a numpy array\n",
        "    return np.array(vectors)\n",
        "\n",
        "# Step 3: Generate FastText vectors for training and testing data\n",
        "# Use the previously defined function to convert tokenized sentences to vectors\n",
        "# by averaging the word vectors learned by the FastText model.\n",
        "X_train_fasttext = get_w2v_vectors(X_train_tokenized, fasttext_model)\n",
        "X_test_fasttext = get_w2v_vectors(X_test_tokenized, fasttext_model)\n",
        "\n",
        "# Step 4: Train machine learning models (Logistic Regression and Random Forest)\n",
        "# on FastText sentence vectors and evaluate their performance\n",
        "for model_name, model in models.items():\n",
        "    # Train the model on FastText vectors and the corresponding labels\n",
        "    model.fit(X_train_fasttext, y_train)\n",
        "    # Calculate accuracy on the training and testing data\n",
        "    train_acc = model.score(X_train_fasttext, y_train)\n",
        "    test_acc = model.score(X_test_fasttext, y_test)\n",
        "\n",
        "    # Store the model name and its accuracy results\n",
        "    results['Model'].append(model_name + \" (FastText)\")  # Append model name with FastText notation\n",
        "    results['Train Accuracy'].append(train_acc)  # Append training accuracy\n",
        "    results['Test Accuracy'].append(test_acc)  # Append testing accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0axr0tDWqS6",
        "outputId": "e4bdb2c3-cb8b-47cd-bdbe-3253cd9d1cf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Model': ['Logistic Regression (Word2Vec)', 'Random Forest (Word2Vec)', 'Logistic Regression (FastText)', 'Random Forest (FastText)'], 'Train Accuracy': [0.683874139626352, 0.9783677482792527, 0.7340216322517208, 0.9783677482792527], 'Test Accuracy': [0.6400591278640059, 0.6577974870657798, 0.6858832224685883, 0.6962305986696231]}\n"
          ]
        }
      ],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfEyfexIWqP4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDtZU63DWnUO"
      },
      "source": [
        "### 3) GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvn8KHPvXTfM",
        "outputId": "2881562b-19c7-44f3-faf9-4b25e10819f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: glove-python3 in /home/minjoo/minjoo/lib/python3.9/site-packages (0.1.0)\n",
            "Requirement already satisfied: numpy in /home/minjoo/minjoo/lib/python3.9/site-packages (from glove-python3) (1.26.4)\n",
            "Requirement already satisfied: scipy in /home/minjoo/minjoo/lib/python3.9/site-packages (from glove-python3) (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install glove-python3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcEJ0GzEWpj6",
        "outputId": "bfe6a773-ed64-4b12-f5c9-07999e00504b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing 10 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n"
          ]
        }
      ],
      "source": [
        "# Import GloVe libraries to create word embeddings using the GloVe algorithm\n",
        "from glove import Corpus, Glove\n",
        "\n",
        "# Step 1: Train GloVe model\n",
        "# Create a Corpus object that will hold the co-occurrence matrix for the GloVe model\n",
        "corpus = Corpus()\n",
        "\n",
        "# Fit the corpus with the tokenized training data\n",
        "# The 'window' parameter specifies the context window size around each word\n",
        "corpus.fit(X_train_tokenized, window=5)\n",
        "\n",
        "# Initialize the GloVe model\n",
        "# no_components: Dimensionality of the word vectors\n",
        "# learning_rate: Learning rate for model training\n",
        "glove_model = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "# Train the GloVe model using the co-occurrence matrix from the corpus\n",
        "# epochs: Number of iterations to train the model\n",
        "# no_threads: Number of parallel threads for training\n",
        "# verbose: Whether to print progress during training\n",
        "glove_model.fit(corpus.matrix, epochs=10, no_threads=4, verbose=True)\n",
        "\n",
        "# Add the word dictionary from the corpus to the GloVe model\n",
        "# This allows the model to map words to indices in the vector space\n",
        "glove_model.add_dictionary(corpus.dictionary)\n",
        "\n",
        "# Step 2: Function to generate sentence vectors from GloVe model\n",
        "# The function converts tokenized sentences into sentence vectors\n",
        "# by averaging the GloVe word vectors for words found in the dictionary\n",
        "def get_glove_vectors(data, model, dictionary, vector_size=100):\n",
        "    vectors = []\n",
        "    for sentence in data:\n",
        "        # Initialize a zero vector for each sentence\n",
        "        sentence_vec = np.zeros(vector_size)\n",
        "        count = 0  # Count the number of valid words in the model\n",
        "        for word in sentence:\n",
        "            # Check if the word exists in the dictionary\n",
        "            if word in dictionary:\n",
        "                # Add the word's GloVe vector to the sentence vector\n",
        "                sentence_vec += model.word_vectors[dictionary[word]]\n",
        "                count += 1\n",
        "        # If the sentence has valid words, average their word vectors\n",
        "        if count != 0:\n",
        "            # Compute the average of the word vectors to form the sentence vector\n",
        "            sentence_vec /= count\n",
        "        # Append the resulting sentence vector to the list\n",
        "        vectors.append(sentence_vec)\n",
        "    # Return the sentence vectors as a numpy array\n",
        "    return np.array(vectors)\n",
        "\n",
        "# Generate GloVe vectors for both training and testing data\n",
        "# This uses the get_glove_vectors function to convert tokenized data into sentence vectors\n",
        "X_train_glove = get_glove_vectors(X_train_tokenized, glove_model, corpus.dictionary)\n",
        "X_test_glove = get_glove_vectors(X_test_tokenized, glove_model, corpus.dictionary)\n",
        "\n",
        "# Step 3: Model training and performance evaluation\n",
        "# Train each machine learning model (Logistic Regression, Random Forest)\n",
        "# on the GloVe sentence vectors and evaluate their performance\n",
        "for model_name, model in models.items():\n",
        "    # Fit the model using the GloVe vectors and corresponding labels\n",
        "    model.fit(X_train_glove, y_train)\n",
        "    # Calculate accuracy on both training and testing datasets\n",
        "    train_acc = model.score(X_train_glove, y_train)\n",
        "    test_acc = model.score(X_test_glove, y_test)\n",
        "\n",
        "    # Store the model name and accuracy results\n",
        "    results['Model'].append(model_name + \" (GloVe)\")  # Append model name with GloVe notation\n",
        "    results['Train Accuracy'].append(train_acc)  # Append training accuracy\n",
        "    results['Test Accuracy'].append(test_acc)  # Append testing accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KFN1XSxXQ59",
        "outputId": "9b16abe0-04d2-4cb8-9b77-85eec8e97a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Model': ['Logistic Regression (Word2Vec)', 'Random Forest (Word2Vec)', 'Logistic Regression (FastText)', 'Random Forest (FastText)', 'Logistic Regression (GloVe)', 'Random Forest (GloVe)'], 'Train Accuracy': [0.683874139626352, 0.9783677482792527, 0.7340216322517208, 0.9783677482792527, 0.47640117994100295, 0.9783677482792527], 'Test Accuracy': [0.6400591278640059, 0.6577974870657798, 0.6858832224685883, 0.6962305986696231, 0.4656319290465632, 0.5898004434589801]}\n"
          ]
        }
      ],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "a_p1xp6-Y-vy",
        "outputId": "95325f55-ffef-4643-8fb6-8987328eb826"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Train Accuracy</th>\n",
              "      <th>Test Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression (Word2Vec)</td>\n",
              "      <td>0.683874</td>\n",
              "      <td>0.640059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Random Forest (Word2Vec)</td>\n",
              "      <td>0.978368</td>\n",
              "      <td>0.657797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logistic Regression (FastText)</td>\n",
              "      <td>0.734022</td>\n",
              "      <td>0.685883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest (FastText)</td>\n",
              "      <td>0.978368</td>\n",
              "      <td>0.696231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Logistic Regression (GloVe)</td>\n",
              "      <td>0.476401</td>\n",
              "      <td>0.465632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Random Forest (GloVe)</td>\n",
              "      <td>0.978368</td>\n",
              "      <td>0.589800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Model  Train Accuracy  Test Accuracy\n",
              "0  Logistic Regression (Word2Vec)        0.683874       0.640059\n",
              "1        Random Forest (Word2Vec)        0.978368       0.657797\n",
              "2  Logistic Regression (FastText)        0.734022       0.685883\n",
              "3        Random Forest (FastText)        0.978368       0.696231\n",
              "4     Logistic Regression (GloVe)        0.476401       0.465632\n",
              "5           Random Forest (GloVe)        0.978368       0.589800"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Organize the results into a dataframe for better visibility\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Print the results in a table format\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBrGeuTJcG7l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "minjoo",
      "language": "python",
      "name": "iitpy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
