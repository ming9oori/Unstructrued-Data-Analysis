{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAS20fuhEBqW"
      },
      "source": [
        "# 3. Document Classification Based on BOW\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M159n1s3EBqa"
      },
      "source": [
        "## 1. Preparing the 20 Newsgroups Data and Feature Extraction\n",
        "\n",
        " http://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP8xqhbdEBqc",
        "outputId": "e538f233-b382-4368-b475-002101e2bbd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#Train set size: 2034\n",
            "#Test set size: 1353\n",
            "#Selected categories: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n",
            "#Train labels: {0, 1, 2, 3}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Create a list of topics to select from the 20 categories\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "\n",
        "# Fetch the training dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',\n",
        "# Remove hinting parts from the email content - classify purely based on content\n",
        "                                      remove=('headers', 'footers', 'quotes'),\n",
        "                                      categories=categories)\n",
        "\n",
        "# Fetch the test dataset\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',\n",
        "                                     remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=categories)\n",
        "\n",
        "print('#Train set size:', len(newsgroups_train.data))\n",
        "print('#Test set size:', len(newsgroups_test.data))\n",
        "print('#Selected categories:', newsgroups_train.target_names)\n",
        "print('#Train labels:', set(newsgroups_train.target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyOgG7L3EBqh",
        "outputId": "563bb290-8e39-4d01-ca88-e71646c3f158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#Train set text samples: Hi,\n",
            "\n",
            "I've noticed that if you only save a model (with all your mapping planes\n",
            "positioned carefully) to a .3DS file that when you reload it after restarting\n",
            "3DS, they are given a default position and orientation.  But if you save\n",
            "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
            "know why this information is not stored in the .3DS file?  Nothing is\n",
            "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
            "I'd like to be able to read the texture rule information, does anyone have \n",
            "the format for the .PRJ file?\n",
            "\n",
            "Is the .CEL file format available from somewhere?\n",
            "\n",
            "Rych\n",
            "#Train set label smaples: 1\n",
            "#Test set text samples: TRry the SKywatch project in  Arizona.\n",
            "#Test set label smaples: 2\n"
          ]
        }
      ],
      "source": [
        "print('#Train set text samples:', newsgroups_train.data[0])\n",
        "print('#Train set label smaples:', newsgroups_train.target[0])\n",
        "print('#Test set text samples:', newsgroups_test.data[0])\n",
        "print('#Test set label smaples:', newsgroups_test.target[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kJl9NpjJEBqj"
      },
      "outputs": [],
      "source": [
        "X_train = newsgroups_train.data   # Training dataset documents\n",
        "y_train = newsgroups_train.target # Training dataset labels\n",
        "\n",
        "X_test = newsgroups_test.data     # Test dataset documents\n",
        "y_test = newsgroups_test.target   # Test dataset labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icIDPfDrDAL1"
      },
      "source": [
        "## 2. Document Representation Based on Distributed Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QEQdqWrV166"
      },
      "source": [
        "### 1) Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YuEgpDpD9d2",
        "outputId": "5876d575-3456-4bdf-be3f-038225f35fb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/minjoo/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/minjoo/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for Word2Vec and machine learning models\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK resources for tokenization and stopwords\n",
        "nltk.download('punkt')  # For word tokenization\n",
        "nltk.download('stopwords')  # For filtering out common stopwords\n",
        "\n",
        "# Initialize a set of stopwords for English and a stemmer to reduce words to their root form\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()  # PorterStemmer is commonly used to reduce words to their stem form\n",
        "\n",
        "# Data preprocessing function\n",
        "def preprocess_data(data):\n",
        "    processed_data = []\n",
        "    for sentence in data:\n",
        "        # Tokenize the sentence into words\n",
        "        tokens = word_tokenize(sentence)\n",
        "        # Convert to lowercase, remove stopwords and special characters, and apply stemming\n",
        "        tokens = [stemmer.stem(re.sub(r'\\W+', '', word.lower()))\n",
        "                  for word in tokens\n",
        "                  if word.lower() not in stop_words and re.sub(r'\\W+', '', word)]\n",
        "        processed_data.append(tokens)  # Add the cleaned tokens to the processed data\n",
        "    return processed_data\n",
        "\n",
        "# Step 1: Train the Word2Vec model\n",
        "# Preprocess the training and testing data using the preprocess_data function\n",
        "X_train_tokenized = preprocess_data(X_train)\n",
        "X_test_tokenized = preprocess_data(X_test)\n",
        "\n",
        "# Flatten the tokenized training data into a single list and calculate word frequency\n",
        "all_words = [word for sentence in X_train_tokenized for word in sentence]\n",
        "word_counts = Counter(all_words)  # Count frequency of each word in the training data\n",
        "\n",
        "# Define a threshold to remove low-frequency words\n",
        "min_count_threshold = 2  # Words with a frequency of 2 or lower will be removed\n",
        "# Filter the tokenized training and testing data to keep only frequent words\n",
        "X_train_tokenized = [[word for word in sentence if word_counts[word] > min_count_threshold] for sentence in X_train_tokenized]\n",
        "X_test_tokenized = [[word for word in sentence if word_counts[word] > min_count_threshold] for sentence in X_test_tokenized]\n",
        "\n",
        "# Train the Word2Vec model with the tokenized and filtered training data\n",
        "# vector_size: Dimensionality of the word vectors\n",
        "# window: Maximum distance between the current and predicted word\n",
        "# sg: Use skip-gram (1) instead of CBOW (0)\n",
        "w2v_model = Word2Vec(sentences=X_train_tokenized, vector_size=100, window=2, min_count=2, sg=1)\n",
        "\n",
        "# Step 2: Function to generate Word2Vec vectors for each sentence\n",
        "# Each sentence vector is computed as the average of its word vectors\n",
        "# The function calculates each sentence's vector by averaging the vectors of the words present in the Word2Vec model. It sums the vectors for the valid words in the sentence and divides by the total number of those words to find the average, resulting in a single vector representation for the sentence.\n",
        "\n",
        "def get_w2v_vectors(data, model, vector_size=100):\n",
        "    vectors = []\n",
        "    for sentence in data:\n",
        "        # Initialize a zero vector for the sentence\n",
        "        sentence_vec = np.zeros(vector_size)\n",
        "        count = 0  # Track the number of words found in the Word2Vec model\n",
        "        for word in sentence:\n",
        "            # Check if the word exists in the Word2Vec model\n",
        "            if word in model.wv.key_to_index:\n",
        "                # Add the word vector to the sentence vector\n",
        "                sentence_vec += model.wv[word]\n",
        "                count += 1\n",
        "        # If the sentence contains valid words, compute the average of the word vectors\n",
        "        if count != 0:\n",
        "            # The sentence vector is the average of the word vectors in the sentence\n",
        "            sentence_vec /= count\n",
        "        # Append the resulting sentence vector (average of word vectors) to the list\n",
        "        vectors.append(sentence_vec)\n",
        "    return np.array(vectors)\n",
        "\n",
        "# Generate Word2Vec vectors for both the training and testing data\n",
        "X_train_w2v = get_w2v_vectors(X_train_tokenized, w2v_model)\n",
        "X_test_w2v = get_w2v_vectors(X_test_tokenized, w2v_model)\n",
        "\n",
        "# Step 3: Train machine learning models (Logistic Regression and Random Forest)\n",
        "# Initialize two different classifiers for comparison\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),  # Logistic Regression with maximum iterations set to 1000\n",
        "    'Random Forest': RandomForestClassifier()  # Random Forest Classifier\n",
        "}\n",
        "\n",
        "# Dictionary to store model names and their performance metrics\n",
        "results = {'Model': [], 'Train Accuracy': [], 'Test Accuracy': []}\n",
        "\n",
        "# Train each model and evaluate accuracy on the training and testing data\n",
        "for model_name, model in models.items():\n",
        "    # Fit the model using the Word2Vec vectorized training data and corresponding labels\n",
        "    model.fit(X_train_w2v, y_train)\n",
        "    # Calculate accuracy on both the training and testing datasets\n",
        "    train_acc = model.score(X_train_w2v, y_train)\n",
        "    test_acc = model.score(X_test_w2v, y_test)\n",
        "\n",
        "    # Store the results for each model\n",
        "    results['Model'].append(model_name + \" (Word2Vec)\")  # Add the model name and the method used (Word2Vec)\n",
        "    results['Train Accuracy'].append(train_acc)  # Training accuracy\n",
        "    results['Test Accuracy'].append(test_acc)  # Testing accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yniy_RswYVfi",
        "outputId": "21eb9a21-10a9-4f88-ca83-fb5933937bf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Model': ['Logistic Regression (Word2Vec)', 'Random Forest (Word2Vec)'], 'Train Accuracy': [0.683874139626352, 0.9783677482792527], 'Test Accuracy': [0.6400591278640059, 0.6577974870657798]}\n"
          ]
        }
      ],
      "source": [
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBrGeuTJcG7l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "minjoo",
      "language": "python",
      "name": "iitpy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
